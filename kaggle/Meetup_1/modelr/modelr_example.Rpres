xwMOOC R Meetup 1회차
========================================================
author: Sang Yeol Lee
date: `r format(Sys.Date(), format="%B %d %Y")`
width: 1500 
height: 1800
transition: linear
transition-speed: slow
autosize: true

<h3>
- 대상자 : R를 사용하고 좋아하는 참가자 누구나
- 주제 : Tidyverse - modelr

- 운영 지원: [캐글뽀개기](https://www.facebook.com/groups/kagglebreak) 

- 장소 지원: [kosslab](https://kosslab.kr/)


- [페북 이벤트 링크](https://www.facebook.com/events/1822675967748595)

- [dropbox 자료 다운로드 링크](https://www.dropbox.com/sh/0n3650m6ifpdjxn/AAArgiJz5LnNdVTNUhpxjpGza?dl=0)

========================================================
id: slide1
type: prompt

# Tidyverse -- Modelr
  <h2>
  - * modelr 기능 소개 (2/6h)
  - * 캐글 문제 소개 및 적용 (4/6h)
  - * 회귀분석 문제 적용 (생략)
  - * Q&A 

<br>

## 자료참고 레퍼런스
  [R4DS](http://r4ds.had.co.nz/1)
  
  [Tidyverse](https://www.tidyverse.org/packages/)
  
  [Pyconkr 2017 kaggle tutorial](https://github.com/KaggleBreak/walkingkaggle/blob/master/pycon2017_kr/pycon_korea_2017_Kaggle_tutorial.ipynb)

<br>

========================================================
## 1단계 : 라이브러리 소개 (modelr)
[Go to slide 1](#/slide1)

- Modelling Functions that Work with the Pipe

![Index.html](./img/img1.png)

- 원활한 통합을 지원하는 모델링을 위한 함수
데이터 조작 및 시각화의 파이프 라인으로 모델링

========================================================

## 1단계 라이브러리 설치 및 기본기능 소개

### Partitioning and sampling

```{r}
#install.packages('tidyverse', dependencies = T, repos='http://cran.r-project.org')
#install.packages('modelr', dependencies = T, repos='http://cran.r-project.org')

library(tidyverse)
library(modelr)

# a subsample of the first ten rows in the data frame
rs <- resample(mtcars, 1:10)
as.data.frame(rs)

rs2 <- resample(mtcars, 1:10)
as.data.frame(rs2)
```

==================
## 1단계 기본기능 소개
### Partitioning and sampling

```{r}
ex <- resample_partition(mtcars, c(test = 0.3, train = 0.7))
ex
lapply(ex, dim)
```

==================
## 1단계 기본기능 소개
### Partitioning and sampling

```{r}
# bootstrap
boot <- bootstrap(mtcars, 100)
boot
dim(mtcars)
boot$strap[[1]]
dim(boot$strap[[1]])

# k-fold cross-validation
cv1 <- crossv_kfold(mtcars, 5)
cv1

dim(cv1$train[[1]])
dim(cv1$test[[1]])

```

==================
## 1단계 기본기능 소개
### Partitioning and sampling

```{r}
# Monte Carlo cross-validation
cv2 <- crossv_mc(mtcars, 100)
cv2

dim(cv2$train[[1]])
dim(cv2$test[[1]])
```

==================
## 1단계 기본기능 소개
### Model quality metrics

```{r}
#modelr includes several often-used model quality metrics:
mod <- lm(mpg ~ wt, data = mtcars) #mpg : Miles/US gallon, wt (weight)
rmse(mod, mtcars) #root-mean-squared-error
rsquare(mod, mtcars) #rsquare is the variance of the predictions divided by by the variance of the response

mae(mod, mtcars) #the mean absolute error
qae(mod, mtcars) #quantiles of absolute error
```

==================
## 1단계 기본기능 소개
### Interacting with models

```{r}
#A set of functions let you seamlessly add predictions and residuals as additional columns to an existing data frame:
df <- tibble::data_frame(
  x = sort(runif(100)),
  y = 5 * x + 0.5 * x ^ 2 + 3 + rnorm(length(x))
)

mod <- lm(y ~ x, data = df)
df %>% add_predictions(mod)
df %>% add_residuals(mod)
```

==================
## 1단계 기본기능 소개
### Interacting with models

```{r}
#For visualization purposes it is often useful to use an evenly spaced grid of points from the data:
data_grid(mtcars, wt = seq_range(wt, 10), cyl, vs)

# For continuous variables, seq_range is useful
mtcars_mod <- lm(mpg ~ wt + cyl + vs, data = mtcars)
data_grid(mtcars, wt = seq_range(wt, 10), cyl, vs) %>% add_predictions(mtcars_mod)

```


========================================================
## 2단계 : 캐글 소개 및 적용
[Go to slide 1](#/slide1)

- [Kaggle](https://www.kaggle.com/)

  - Kaggle은 기업 및 연구원이 데이터 및 통계를 게시하고 데이터 마이너가 예측 및 설명을 위한 최상의 모델을 만들기 위해 경쟁하는 예측 모델링 및 분석 경쟁을위한 플랫폼. 

  - Crowdsourcing 접근 방식은 예측 모델링 작업에 적용 할 수있는 무수한 전략이 있으며 어떤 기술 또는 분석가가 가장 효과적인지를 처음부터 알 수 없다는 사실에 의존.

<br>

- [뉴욕시 임대아파트 문제](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries) 

  - RentHop은 데이터를 사용하여 임대 목록을 품질 별로 분류하여 아파트 검색을 더 똑똑하게 만드는 사이트
  
  - 완벽한 아파트를 찾긴 어렵지만 사용 가능한 모든 부동산 데이터를 프로그래밍 방식으로 구조화하고 이해하는 것이 더욱 어려움.
  
![RentHop](https://camo.githubusercontent.com/92bded67b056d8f78a0121e8b17e39777a100b82/68747470733a2f2f6b6167676c65322e626c6f622e636f72652e77696e646f77732e6e65742f636f6d7065746974696f6e732f6b6167676c652f353539302f6d656469612f74776f7369676d612d72656e74686f702d62616e6e65722d323530783235302e706e67)


- [Renthop](https://www.renthop.com/)

  - RentHop은 사용자가 뉴욕, 보스턴, 시카고 및 다른 대도시 지역의 아파트를 검색 할 수있게 해주는 웹 및 모바일 기반 검색 엔진. 이 사이트는 실시간 리스팅을 제공하며 품질에 따라 아파트를 분류하는 고유 한 정렬 알고리즘을 사용
  
![img](https://github.com/KaggleBreak/walkingkaggle/raw/bb5aefa72dd3e930ad618f8a733836c507353edd/pycon2017_kr/img/renthop_1.png)

==================
## 2단계 캐글 소개 및 적용

### 목표 : modelr 기능 일부를 캐글에 사용하자. (귀찮은 CV...)

<br>

## 데이터 소개

### File descriptions
- train.json - the training set
- test.json - the test set
- sample_submission.csv - a sample submission file in the correct format
- images_sample.zip - listing images organized by listing_id (a sample of 100 listings)
- Kaggle-renthop.7z - (optional) listing images organized by listing_id. Total size: 78.5GB compressed. Distributed by BitTorrent (Kaggle-renthop.torrent).

<br>

### Data fields
- bathrooms: number of bathrooms
- bedrooms: number of bathrooms
- listing_id: 포스팅 ID
- building_id: 건물 ID
- manager_id: 포스팅 게시자 ID
- created: 포스팅 된 시각 (UTC 기준으로 생각됨)
- latitude: 위도
- longitude: 경도
- price: in USD
- features: a list of features about this apartment
- photos: a list of photo links. You are welcome to download the pictures yourselves from renthop's site, but they are the same as imgs.zip.
- display_address: 주소 (도로명까지)
- street_address: 주소 (번지까지) 

### - interest_level: this is the target variable. It has 3 categories: 'high', 'medium', 'low'

[데이터 다운로드 링크](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/data)

- 데이터 다운로드 받으려면 캐글 가입 및 대회 동의를 해야함. 가입할 때 인증이 필요


==================
## 2단계 캐글 소개 및 적용


## 윈도우 경우

### [1. rJava  설치](https://thebook.io/006723/ch09/02/03/02-2/)

### 2 .xgboost 설치
[1. mingw-w64 다운로드 및 설치](https://sourceforge.net/projects/mingw-w64/)
  
[2.윈도우에 XGBoost 설치하기](http://quantfactory.blogspot.kr/2017/04/xgboost.html)

```{r}
#install.packages(c('MLmetrics', 'rjson', 'tidyverse', 'knitr', 'purrr', 'stringr', 'lubridate', 'rJava', 'sqldf', 'modelr', 'ModelMetrics', 'caret'))
#install.packages('xgboost')

require(rjson)
require(dplyr)
require(purrr)
require(knitr)
require(stringr)
require(lubridate)
require(tidyr)
require(rJava)
require(sqldf)
require(modelr)
require(ModelMetrics)
require(caret)
require(xgboost)

options(tibble.width = Inf) #tibble 출력 컬럼 변경
```

==================
## 2단계 캐글 소개 및 적용

### 데이터 로딩
```{r}
train = fromJSON(file = "./data/train.json")
test = fromJSON(file = "./data/test.json")

column <- setdiff(names(train), c("photos", "features")) #photos, features는 리스트 형태로 저장하기 위해서
column

train <- map_at(train, column, unlist) %>% tibble::as_tibble(.)

test <- map_at(test, column, unlist) %>% tibble::as_tibble(.)

colnames(train)
glimpse(train) #tibble 형태에서 str과 유사한 기능
```

==================
## 2단계 캐글 소개 및 적용

### 데이터 전처리 및 변수 추가

```{r}
train <- train %>% mutate(
                  bathrooms = as.numeric(bathrooms),
                  bedrooms = as.numeric(bedrooms),
                  created = ymd_hms(created),
                  latitude = as.numeric(latitude),
                  longitude = as.numeric(longitude),
                  price = as.numeric(price),
                  created_week = weeks(created),
                  created_hour = hour(created),
                  created_weekday = wday(created),
                  created_month = month(created),
                  price_t = price / bedrooms,
                  feature_count = lengths(features),
                  photo_count = lengths(photos),
                  description_count = lengths(str_split(description, "\\s+")),
                  interest_level = factor(interest_level, levels=c('low', 'medium', 'high')))

test <- test %>% mutate(bathrooms = as.numeric(bathrooms),
                          bedrooms = as.numeric(bedrooms),
                          created = ymd_hms(created),
                          latitude = as.numeric(latitude),
                          longitude = as.numeric(longitude),
                          price = as.numeric(price),
                          created_week = weeks(created),
                          created_hour = hour(created),
                          created_weekday = wday(created),
                          created_month = month(created),
                          price_t = price / bedrooms,
                          feature_count = lengths(features),
                          photo_count = lengths(photos),
                          description_count = lengths(str_split(description, "\\s+"))
                          )
features_to_use = c("bathrooms", "bedrooms", "latitude", "longitude", "price", "price_t", "photo_count", "feature_count", "description_count", "created_week", "created_hour", "created_weekday", "created_month", "listing_id")

features_to_use
glimpse(train) 
```

==================
## 2단계 캐글 소개 및 적용

### 데이터 전처리 및 변수 추가

```{r}
manager_count <- train %>% 
          group_by(manager_id, interest_level) %>%
          summarise(
            count = n()
          )

manager_count[18:19,]

manager_ratio <- sqldf("
        select manager_id, (low_count*1.0)/sum_count low_ratio
                           , (medium_count*1.0)/sum_count medium_ratio 
                           , (high_count*1.0)/sum_count high_ratio
                           , sum_count
        from (
        select manager_id, 
        sum(case when interest_level == 'low' then count*1.0 else 0 end) as low_count,
        sum(case when interest_level == 'medium' then count*1.0 else 0 end) as medium_count, 
        sum(case when interest_level == 'high' then count*1.0 else 0 end) as high_count, 
        sum(count) sum_count
        from manager_count
        group by manager_id
        )t")

manager_ratio %>% filter(manager_id == '77f81a0a8af6db8349587acefd1b533f')

train <- train %>% left_join(manager_ratio, by = 'manager_id')
test <- test %>% left_join(manager_ratio, by = 'manager_id')
```

==================
## 2단계 캐글 소개 및 적용

### 데이터 전처리 및 변수 추가

```{r}
train <- train %>% mutate(
  display_address = as.integer(as.factor(display_address)),
  manager_id = as.integer(as.factor(manager_id)),
  building_id = as.integer(as.factor(building_id)),
  street_address = as.integer(as.factor(street_address)),
)

test <- test %>% mutate(
  display_address = as.integer(as.factor(display_address)),
  manager_id = as.integer(as.factor(manager_id)),
  building_id = as.integer(as.factor(building_id)),
  street_address = as.integer(as.factor(street_address)),
)

features_to_use = c(features_to_use,'low_ratio', 'medium_ratio', 'high_ratio', 'sum_count', 'display_address', 'manager_id', 'building_id', 'street_address')
features_to_use
```


==================
## 2단계 캐글 소개 및 적용

### 머신러닝 모델링
[Kaggle Ref](https://flonelin.wordpress.com/2016/07/26/tuning-xgboostextream-gradient-boosting/)

```{r}
runXGB <- function(train_X, train_y, test_X, test_y=NULL, feature_names=NULL, seed_val=0, num_rounds=100){
    param <- list()
    param['objective'] = 'multi:softprob'
    param['eta'] = 0.03
    param['max_depth'] = 6
    param['silent'] = 1
    param['num_class'] = 3
    param['eval_metric'] = "mlogloss"
    param['min_child_weight'] = 1
    param['subsample'] = 0.7 # 70프로만 뽑겠다
    param['colsample_bytree'] = 0.7 # 변수 컬럼 비율
    param['seed'] = seed_val # 초기값 설정 랜덤 안되게
    num_rounds = num_rounds

    plst = param
    xgtrain = xgb.DMatrix(data=train_X, label=train_y) 
    feature_names = dimnames(xgtrain)

    # test의 타겟값을 넣은 xgb model
    if (!is.null(test_y)){
      xgtest = xgb.DMatrix(data=test_X, label=test_y)
      watchlist = list('train'=xgtrain, 'test'=xgtest)
      model = xgb.train(params=plst, data=xgtrain, nrounds=num_rounds, watchlist=watchlist, early_stopping_rounds=20)
    }
    else{
      xgtest = xgb.DMatrix(test_X)
      model = xgb.train(plst, xgtrain, num_rounds)
    }
    pred_test_y = predict(model, xgtest)
    return(list(pred_test_y, model, feature_names))
}
```

==================
## 2단계 캐글 소개 및 적용

### 머신러닝 모델링

```{r}
cv_scores = list()
train_df <- train[features_to_use]
train_y <- train['interest_level'] %>% mutate(interest_level = as.numeric(interest_level)-1)


kf <- crossv_kfold(train_df, 5) #modelr 사용!!!

kf[[1]][1]
kf[[2]][1]
```


==================
## 2단계 캐글 소개 및 적용

### 머신러닝 모델링

```{r}
#caret 사용시
kf <- createFolds(1:dim(train_y)[1], k = 5)
str(kf)

#modelr crossv_mc 사용시 
#Monte Carlo cross-validation
kf <- crossv_mc(train_df, 5)
kf

#modelr crossv_kfold 사용시 
kf <- crossv_kfold(train_df, 5)
kf
```

- caret과 비교했을 때 cross validatoin 기능이 비슷하지만 차이가 있음

- caret는 train 모델에 컨트롤 옵션에서 cv를 할 수 있도록 권장하고 있음 (repeatedcv등)

- modelr은 column-list 방식을 제공하고 있어서 좀 더 범용적으로 사용하기 쉬움

==================
## 2단계 캐글 소개 및 적용

### 머신러닝 모델링

```{r}
for (i in 1:5){
  dev_x <- as.matrix(train_df[kf$train[[i]]$idx,])
  val_x <- as.matrix(train_df[kf$test[[i]]$idx,])
  dev_y <- as.matrix(train_y[kf$train[[i]]$idx,])
  val_y <- as.matrix(train_y[kf$test[[i]]$idx,])
  results <- runXGB(dev_x, dev_y, val_x, val_y)
  cv_scores <- append(cv_scores, mlogLoss(actual = as.factor(val_y),
                                          predicted = matrix(results[[1]],ncol=3, byrow=T)))
  print(cv_scores)
  break
}
```

==================
## 2단계 캐글 소개 및 적용

### 대회 제출

```{r}
test_df <- test[features_to_use]

result_test = runXGB(as.matrix(train_df), as.matrix(train_y), as.matrix(test_df), num_rounds=100)
out_df = as.data.frame(matrix(result_test[[1]], ncol=3, byrow=T))
colnames(out_df) = c("low", "medium", "high")
out_df['listing_id'] <- test_df %>% select(listing_id)
write.csv(out_df, file='data/xgb_starter_num_rounds100_xwmooc_1meetup.csv', row.names = FALSE)
```

![제출 결과](./img/img2.png)


==================

# The End
## modelr을 캐글에 적용하기

- 이상열 : syleeie@gmail.com

- 캐글즐기기 매주 수요일 (파트4 종료, https://github.com/KaggleBreak/analyticstool)

- 텐서뽀개기 격주 화요일 (6/27일 파트1 시작,  https://github.com/KaggleBreak/tensorbreak)

[이벤트 링크](https://www.facebook.com/events/1945163935730064)

- 워킹캐글 주말 (8/26일 파트2 시작, https://github.com/KaggleBreak/walkingkaggle)

[이벤트 링크](https://www.facebook.com/events/1472119989567282)

- Q&A?

==================
## Appendix

<br>

### R4DS 24, 25장 요약
- model_building
- many_models
- 회귀 모형 위주로 column-list 여러가지 방법에 대해 자세히 나와 있음. 함수형 프로그램, 분석 자동화

[R4DS](https://github.com/KaggleBreak/analyticstool/blob/master/part4/R/analytics/20170719/23_24.model_building_many_models.nb.html)

- [broom 소개](https://cran.r-project.org/web/packages/broom/vignettes/broom.html)

- Convert statistical analysis objects from R into tidy data frames, so that they can more easily be combined, reshaped and otherwise processed with tools like 'dplyr', 'tidyr' and 'ggplot2'